{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Reviews Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a class Review!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentiment:\n",
    "    NEGATIVE = 'NEGATIVE'\n",
    "    POSITIVE = 'POSITIVE'\n",
    "    NEUTRAL = 'NEUTRAL'\n",
    "\n",
    "class Review:\n",
    "    def __init__(self, text, score):\n",
    "        self.text = text\n",
    "        self.score = score    \n",
    "        self.sentiment = self.get_sentiment()\n",
    "        \n",
    "    def get_sentiment(self):\n",
    "        if self.score <= 2:\n",
    "            return Sentiment.NEGATIVE\n",
    "        elif self.score >=4:\n",
    "            return Sentiment.POSITIVE\n",
    "        else:\n",
    "            return Sentiment.NEUTRAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data\n",
    "Let's import the json data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Love the book, great story line, keeps you entertained.for a first novel from this author she did a great job,  Would definitely recommend!\n",
      "4.0\n",
      "POSITIVE\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "file_name = './dataset/Books_small.json'\n",
    "reviews = []\n",
    "with open(file_name) as f:\n",
    "    for line in f:\n",
    "        review = json.loads(line)\n",
    "        reviews.append(Review(review['reviewText'],review['overall']))\n",
    "\n",
    "print(reviews[5].text)\n",
    "print(reviews[5].score)\n",
    "print(reviews[5].sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prep Data\n",
    "The machine models love numerical data rather than text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(reviews, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vivid characters and descriptions. The author has created a tale that grabs your attention and I couldn't put it down. 5.0 POSITIVE\n"
     ]
    }
   ],
   "source": [
    "print(train[0].text, train[0].score, train[0].sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vivid characters and descriptions. The author has created a tale that grabs your attention and I couldn't put it down.\n",
      "POSITIVE\n"
     ]
    }
   ],
   "source": [
    "train_x = [x.text for x in train]\n",
    "train_y = [x.sentiment for x in train]\n",
    "\n",
    "test_x = [x.text for x in test]\n",
    "test_y = [x.sentiment for x in test]\n",
    "\n",
    "print(train_x[0])\n",
    "print(train_y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# vectorizer.fit(train_x)\n",
    "# vectorizer.transform(train_x) ---> SAME AS V\n",
    "\n",
    "train_x_vectors = vectorizer.fit_transform(train_x)\n",
    "\n",
    "# Don't fit the the vectorizer again to test_x\n",
    "test_x_vectors = vectorizer.transform(test_x)\n",
    "\n",
    "train_x_vectors[0].toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Michael Cunningham mesmerizes with the thoughtful, elegant prose that is this book.  The reader becomes so close to its characters...the reader feels what these brothers feel.  Beautiful and tragic...a book that will stay with me for a long, long time.  Thank you again, Mr. Cunningham.  The Hours remains at the top of my list and The Snow Queen is another gift to your readers.\n",
      "POSITIVE\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['POSITIVE'], dtype='<U8')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "clf_svm = svm.SVC(kernel='linear')\n",
    "\n",
    "clf_svm.fit(train_x_vectors, train_y)\n",
    "\n",
    "n=2\n",
    "print(test_x[n])\n",
    "print(test_y[n])\n",
    "\n",
    "clf_svm.predict(test_x_vectors[n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The review of innovation techniques and examples of their application to healthcare problems is absolutely amazing!  The Lean, on the other hand, is sketchy and not convincing.  Still, an informative and a well-written book.\n",
      "POSITIVE\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['POSITIVE'], dtype='<U8')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf_dec = DecisionTreeClassifier()\n",
    "clf_dec.fit(train_x_vectors, train_y)\n",
    "\n",
    "\n",
    "n=42\n",
    "print(test_x[n])\n",
    "print(test_y[n])\n",
    "\n",
    "clf_dec.predict(test_x_vectors[n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An intriguing book, but I am ashamed to admit I sometimes did not fully comprehend what was said, particularly when Chesterton referred to other people (writers) that I am not knowledgeable of.  At any rate a good read.\n",
      "POSITIVE\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['POSITIVE'], dtype='<U8')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "clf_gnb = GaussianNB()\n",
    "# Dense data should be passed and not a Sparse matrix\n",
    "clf_gnb.fit(train_x_vectors.toarray(), train_y) \n",
    "\n",
    "\n",
    "n=5\n",
    "print(test_x[n])\n",
    "print(test_y[n])\n",
    "\n",
    "clf_gnb.predict(test_x_vectors[n].toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I loved this book.  It was very interesting to hear the background involved in training a helper dog and Luis's background in the service.  I learned a lot, but was entertained at the same time.  Of course, I have a a Golden Retriever myself, so I may be biased, but mine is as dumb as a brick!  Great book.\n",
      "POSITIVE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sinjoy/.local/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['POSITIVE'], dtype='<U8')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf_log = LogisticRegression()\n",
    "clf_log.fit(train_x_vectors, train_y) \n",
    "\n",
    "\n",
    "n=6\n",
    "print(test_x[n])\n",
    "print(test_y[n])\n",
    "\n",
    "clf_log.predict(test_x_vectors[n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8242424242424242\n",
      "0.7787878787878788\n",
      "0.8121212121212121\n",
      "0.8303030303030303\n"
     ]
    }
   ],
   "source": [
    "# Mean Accuraies \n",
    "print(clf_svm.score(test_x_vectors, test_y))\n",
    "print(clf_dec.score(test_x_vectors, test_y))\n",
    "print(clf_gnb.score(test_x_vectors.toarray(), test_y))\n",
    "print(clf_log.score(test_x_vectors, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['POSITIVE', 'NEUTRAL', 'NEGATIVE']\n",
      "[0.91319444 0.21052632 0.22222222]\n",
      "[0.88307155 0.13333333 0.        ]\n",
      "[0.89678511 0.08510638 0.09090909]\n",
      "[0.91370558 0.12244898 0.1       ]\n"
     ]
    }
   ],
   "source": [
    "# F1 Scores\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "labels=[Sentiment.POSITIVE, Sentiment.NEUTRAL, Sentiment.NEGATIVE]\n",
    "print(labels)\n",
    "\n",
    "print(f1_score(test_y, clf_svm.predict(test_x_vectors), average=None, labels=labels))\n",
    "print(f1_score(test_y, clf_dec.predict(test_x_vectors), average=None, labels=labels))\n",
    "print(f1_score(test_y, clf_gnb.predict(test_x_vectors.toarray()), average=None, labels=labels))\n",
    "print(f1_score(test_y, clf_log.predict(test_x_vectors), average=None, labels=labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All the models predict POSITIVE correctly but NEUTRAL and NEGATIVE are bad!!\n",
    "Let's find out why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "670\n",
      "552 0.8238805970149253\n",
      "71 0.10597014925373134\n",
      "47 0.07014925373134329\n"
     ]
    }
   ],
   "source": [
    "print(len(train_y))\n",
    "print(train_y.count(Sentiment.POSITIVE),train_y.count(Sentiment.POSITIVE)/len(train_y))\n",
    "print(train_y.count(Sentiment.NEUTRAL),train_y.count(Sentiment.NEUTRAL)/len(train_y))\n",
    "print(train_y.count(Sentiment.NEGATIVE),train_y.count(Sentiment.NEGATIVE)/len(train_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see that there are around 82\\% POSITIVEs in the training data. Thus, our models will be heavily biased towards POSITIVE.\n",
    "Now, there are only 47 NEUTRALs in this data. To balance the dataset, wewill either need to make all three classes around 47 data points each (which is too small for a dataset) OR..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GET A BIGGER DATASET!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I hoped for Mia to have some peace in this book, but her story is so real and raw.  Broken World was so touching and emotional because you go from Mia's trauma to her trying to cope.  I love the way the story displays how there is no \"just bouncing back\" from being sexually assaulted.  Mia showed us how those demons come for you every day and how sometimes they best you. I was so in the moment with Broken World and hurt with Mia because she was surrounded by people but so alone and I understood her feelings.  I found myself wishing I could give her some of my courage and strength or even just to be there for her.  Thank you Lizzy for putting a great character's voice on a strong subject and making it so that other peoples story may be heard through Mia's.\n",
      "5.0\n",
      "POSITIVE\n"
     ]
    }
   ],
   "source": [
    "file_name = './dataset/Books_small_10000.json'\n",
    "reviews = []\n",
    "with open(file_name) as f:\n",
    "    for line in f:\n",
    "        review = json.loads(line)\n",
    "        reviews.append(Review(review['reviewText'],review['overall']))\n",
    "\n",
    "print(reviews[5].text)\n",
    "print(reviews[5].score)\n",
    "print(reviews[5].sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evenly distribute positives and negatives   \n",
    "import random\n",
    "class ReviewContainer:\n",
    "    def __init__(self, reviews):\n",
    "        self.reviews = reviews\n",
    "    \n",
    "    def get_text(self):\n",
    "        return [x.text for x in self.reviews]\n",
    "    \n",
    "    def get_sentiment(self):\n",
    "        return [x.sentiment for x in self.reviews]\n",
    "    \n",
    "    def evenly_distribute(self):\n",
    "        negative = list(filter(lambda x: x.sentiment == Sentiment.NEGATIVE, self.reviews))\n",
    "        positive = list(filter(lambda x: x.sentiment == Sentiment.POSITIVE, self.reviews))      \n",
    "#         print(len(negative))\n",
    "#         print(len(positive))\n",
    "        positive_shrunk = positive[:len(negative)]\n",
    "        self.reviews = negative + positive_shrunk\n",
    "        random.shuffle(self.reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(reviews, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note to Self:  Do for NEUTRAL as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "872\n",
      "416\n"
     ]
    }
   ],
   "source": [
    "train_cont = ReviewContainer(train)\n",
    "test_cont = ReviewContainer(test)\n",
    "\n",
    "train_cont.evenly_distribute() # If test data is not evenly dist. then\n",
    "test_cont.evenly_distribute()  # the NEGATIVE F1 score doesn't improve. WHY??\n",
    "\n",
    "print(len(train_cont.reviews))\n",
    "print(len(test_cont.reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "436\n",
      "436\n"
     ]
    }
   ],
   "source": [
    "train_x = train_cont.get_text()\n",
    "train_y = train_cont.get_sentiment()\n",
    "\n",
    "test_x = test_cont.get_text()\n",
    "test_y = test_cont.get_sentiment()\n",
    "\n",
    "print(train_y.count(Sentiment.POSITIVE))\n",
    "print(train_y.count(Sentiment.NEGATIVE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "872\n",
      "436 0.5\n",
      "0 0.0\n",
      "436 0.5\n"
     ]
    }
   ],
   "source": [
    "print(len(train_y))\n",
    "print(train_y.count(Sentiment.POSITIVE),train_y.count(Sentiment.POSITIVE)/len(train_y))\n",
    "print(train_y.count(Sentiment.NEUTRAL),train_y.count(Sentiment.NEUTRAL)/len(train_y))\n",
    "print(train_y.count(Sentiment.NEGATIVE),train_y.count(Sentiment.NEGATIVE)/len(train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "416\n",
      "208 0.5\n",
      "0 0.0\n",
      "208 0.5\n"
     ]
    }
   ],
   "source": [
    "print(len(test_y))\n",
    "print(test_y.count(Sentiment.POSITIVE),test_y.count(Sentiment.POSITIVE)/len(test_y))\n",
    "print(test_y.count(Sentiment.NEUTRAL),test_y.count(Sentiment.NEUTRAL)/len(test_y))\n",
    "print(test_y.count(Sentiment.NEGATIVE),test_y.count(Sentiment.NEGATIVE)/len(test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words Vectorization \n",
    "Let's use TDIDF Vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "train_x_vectors = vectorizer.fit_transform(train_x)\n",
    "\n",
    "# Don't fit the the vectorizer again to test_x\n",
    "test_x_vectors = vectorizer.transform(test_x)\n",
    "\n",
    "train_x_vectors[0].toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I think you should just take the title to heart, and skip the content. Try that experience. It's a little bit like seeing a movie preview and then realizing that the full length film isn't going to give you that much more information.\n",
      "NEGATIVE\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['NEGATIVE'], dtype='<U8')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "clf_svm = svm.SVC(kernel='linear')\n",
    "\n",
    "clf_svm.fit(train_x_vectors, train_y)\n",
    "\n",
    "n=2\n",
    "print(test_x[n])\n",
    "print(test_y[n])\n",
    "\n",
    "clf_svm.predict(test_x_vectors[n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Although the Navajo element was interesting as background and provided a view as to life around a reservation, with its cultural nuances, the story itself was very basic. There were limited twists, if any, and the ending left me feeling like the reading journey was uneventful. Disappointing overall.\n",
      "NEGATIVE\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['POSITIVE'], dtype='<U8')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf_dec = DecisionTreeClassifier()\n",
    "clf_dec.fit(train_x_vectors, train_y)\n",
    "\n",
    "\n",
    "n=42\n",
    "print(test_x[n])\n",
    "print(test_y[n])\n",
    "\n",
    "clf_dec.predict(test_x_vectors[n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I bought it as a refresher, something to read on the plane. It doesn't work on Kindle....tables and text get scrambled. You can work it out but quickly get to a point where the pain is way higher than the payoff.\n",
      "NEGATIVE\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['POSITIVE'], dtype='<U8')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "clf_gnb = GaussianNB()\n",
    "# Dense data should be passed and not a Sparse matrix\n",
    "clf_gnb.fit(train_x_vectors.toarray(), train_y) \n",
    "\n",
    "\n",
    "n=5\n",
    "print(test_x[n])\n",
    "print(test_y[n])\n",
    "\n",
    "clf_gnb.predict(test_x_vectors[n].toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I really wanted to like this book. Everything was there for a great story, it just never came together. The conversation between characters felt stilted and forced, even when the author clearly wasn't meaning for it to. There wasn't enough character development for the supporting characters, but with that said this book seemed to still drag on and on. It was irritating to read and I had to make myself finish it. I believe that this author has a ton of potential.... but needs refining ( based off of this book)\n",
      "NEGATIVE\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['NEGATIVE'], dtype='<U8')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf_log = LogisticRegression()\n",
    "clf_log.fit(train_x_vectors, train_y) \n",
    "\n",
    "\n",
    "n=6\n",
    "print(test_x[n])\n",
    "print(test_y[n])\n",
    "\n",
    "clf_log.predict(test_x_vectors[n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8076923076923077\n",
      "0.6298076923076923\n",
      "0.6610576923076923\n",
      "0.8052884615384616\n"
     ]
    }
   ],
   "source": [
    "# Mean Accuraies \n",
    "print(clf_svm.score(test_x_vectors, test_y))\n",
    "print(clf_dec.score(test_x_vectors, test_y))\n",
    "print(clf_gnb.score(test_x_vectors.toarray(), test_y))\n",
    "print(clf_log.score(test_x_vectors, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['POSITIVE', 'NEUTRAL', 'NEGATIVE']\n",
      "[0.80582524 0.         0.80952381]\n",
      "[0.62439024 0.         0.63507109]\n",
      "[0.65693431 0.         0.66508314]\n",
      "[0.80291971 0.         0.80760095]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sinjoy/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1493: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  average, \"true nor predicted\", 'F-score is', len(true_sum)\n",
      "/home/sinjoy/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1493: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  average, \"true nor predicted\", 'F-score is', len(true_sum)\n",
      "/home/sinjoy/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1493: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  average, \"true nor predicted\", 'F-score is', len(true_sum)\n",
      "/home/sinjoy/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1493: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  average, \"true nor predicted\", 'F-score is', len(true_sum)\n"
     ]
    }
   ],
   "source": [
    "# F1 Scores\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "labels=[Sentiment.POSITIVE, Sentiment.NEUTRAL, Sentiment.NEGATIVE]\n",
    "print(labels)\n",
    "\n",
    "print(f1_score(test_y, clf_svm.predict(test_x_vectors), average=None, labels=labels))\n",
    "print(f1_score(test_y, clf_dec.predict(test_x_vectors), average=None, labels=labels))\n",
    "print(f1_score(test_y, clf_gnb.predict(test_x_vectors.toarray()), average=None, labels=labels))\n",
    "print(f1_score(test_y, clf_log.predict(test_x_vectors), average=None, labels=labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['POSITIVE' 'POSITIVE' 'NEGATIVE' 'NEGATIVE']\n",
      "['POSITIVE' 'POSITIVE' 'NEGATIVE' 'POSITIVE']\n",
      "['NEGATIVE' 'POSITIVE' 'NEGATIVE' 'NEGATIVE']\n",
      "['POSITIVE' 'POSITIVE' 'NEGATIVE' 'NEGATIVE']\n"
     ]
    }
   ],
   "source": [
    "test_set = ['Very good book!', 'A must read for any war buff.',\n",
    "            'It is not bad', 'Horrible Waste of time'] \n",
    "\n",
    "test_set = vectorizer.transform(test_set)\n",
    "\n",
    "print(clf_svm.predict(test_set))\n",
    "print(clf_dec.predict(test_set))\n",
    "print(clf_gnb.predict(test_set.toarray()))\n",
    "print(clf_log.predict(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Our Model (with Grid Search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=SVC(),\n",
       "             param_grid={'C': [1, 2, 4, 8, 16], 'kernel': ('linear', 'rbf')})"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'kernel':('linear', 'rbf'), 'C':[1,2,4,8,16]}\n",
    "\n",
    "svc = svm.SVC()\n",
    "tuned_svm = GridSearchCV(svc, parameters,cv=5)\n",
    "tuned_svm.fit(train_x_vectors, train_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_C</th>\n",
       "      <th>param_kernel</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.210116</td>\n",
       "      <td>0.009128</td>\n",
       "      <td>0.044887</td>\n",
       "      <td>0.001079</td>\n",
       "      <td>1</td>\n",
       "      <td>linear</td>\n",
       "      <td>{'C': 1, 'kernel': 'linear'}</td>\n",
       "      <td>0.845714</td>\n",
       "      <td>0.817143</td>\n",
       "      <td>0.873563</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>0.816092</td>\n",
       "      <td>0.829123</td>\n",
       "      <td>0.027788</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.228531</td>\n",
       "      <td>0.002997</td>\n",
       "      <td>0.053695</td>\n",
       "      <td>0.001238</td>\n",
       "      <td>1</td>\n",
       "      <td>rbf</td>\n",
       "      <td>{'C': 1, 'kernel': 'rbf'}</td>\n",
       "      <td>0.868571</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>0.885057</td>\n",
       "      <td>0.775862</td>\n",
       "      <td>0.816092</td>\n",
       "      <td>0.837117</td>\n",
       "      <td>0.038705</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.215722</td>\n",
       "      <td>0.006513</td>\n",
       "      <td>0.043921</td>\n",
       "      <td>0.001008</td>\n",
       "      <td>2</td>\n",
       "      <td>linear</td>\n",
       "      <td>{'C': 2, 'kernel': 'linear'}</td>\n",
       "      <td>0.828571</td>\n",
       "      <td>0.817143</td>\n",
       "      <td>0.862069</td>\n",
       "      <td>0.810345</td>\n",
       "      <td>0.821839</td>\n",
       "      <td>0.827993</td>\n",
       "      <td>0.018047</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.231588</td>\n",
       "      <td>0.004199</td>\n",
       "      <td>0.054353</td>\n",
       "      <td>0.000959</td>\n",
       "      <td>2</td>\n",
       "      <td>rbf</td>\n",
       "      <td>{'C': 2, 'kernel': 'rbf'}</td>\n",
       "      <td>0.868571</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>0.890805</td>\n",
       "      <td>0.775862</td>\n",
       "      <td>0.821839</td>\n",
       "      <td>0.839415</td>\n",
       "      <td>0.039596</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.225223</td>\n",
       "      <td>0.008298</td>\n",
       "      <td>0.044203</td>\n",
       "      <td>0.001582</td>\n",
       "      <td>4</td>\n",
       "      <td>linear</td>\n",
       "      <td>{'C': 4, 'kernel': 'linear'}</td>\n",
       "      <td>0.828571</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>0.850575</td>\n",
       "      <td>0.804598</td>\n",
       "      <td>0.816092</td>\n",
       "      <td>0.827967</td>\n",
       "      <td>0.016392</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.238724</td>\n",
       "      <td>0.004569</td>\n",
       "      <td>0.057497</td>\n",
       "      <td>0.002302</td>\n",
       "      <td>4</td>\n",
       "      <td>rbf</td>\n",
       "      <td>{'C': 4, 'kernel': 'rbf'}</td>\n",
       "      <td>0.868571</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>0.879310</td>\n",
       "      <td>0.775862</td>\n",
       "      <td>0.821839</td>\n",
       "      <td>0.837117</td>\n",
       "      <td>0.036779</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.224928</td>\n",
       "      <td>0.008383</td>\n",
       "      <td>0.045405</td>\n",
       "      <td>0.001535</td>\n",
       "      <td>8</td>\n",
       "      <td>linear</td>\n",
       "      <td>{'C': 8, 'kernel': 'linear'}</td>\n",
       "      <td>0.828571</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>0.850575</td>\n",
       "      <td>0.804598</td>\n",
       "      <td>0.816092</td>\n",
       "      <td>0.827967</td>\n",
       "      <td>0.016392</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.231566</td>\n",
       "      <td>0.002896</td>\n",
       "      <td>0.053920</td>\n",
       "      <td>0.001242</td>\n",
       "      <td>8</td>\n",
       "      <td>rbf</td>\n",
       "      <td>{'C': 8, 'kernel': 'rbf'}</td>\n",
       "      <td>0.868571</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>0.879310</td>\n",
       "      <td>0.775862</td>\n",
       "      <td>0.821839</td>\n",
       "      <td>0.837117</td>\n",
       "      <td>0.036779</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.220163</td>\n",
       "      <td>0.006604</td>\n",
       "      <td>0.047966</td>\n",
       "      <td>0.005303</td>\n",
       "      <td>16</td>\n",
       "      <td>linear</td>\n",
       "      <td>{'C': 16, 'kernel': 'linear'}</td>\n",
       "      <td>0.828571</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>0.850575</td>\n",
       "      <td>0.804598</td>\n",
       "      <td>0.816092</td>\n",
       "      <td>0.827967</td>\n",
       "      <td>0.016392</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.245850</td>\n",
       "      <td>0.011305</td>\n",
       "      <td>0.056053</td>\n",
       "      <td>0.003503</td>\n",
       "      <td>16</td>\n",
       "      <td>rbf</td>\n",
       "      <td>{'C': 16, 'kernel': 'rbf'}</td>\n",
       "      <td>0.868571</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>0.879310</td>\n",
       "      <td>0.775862</td>\n",
       "      <td>0.821839</td>\n",
       "      <td>0.837117</td>\n",
       "      <td>0.036779</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time param_C  \\\n",
       "0       0.210116      0.009128         0.044887        0.001079       1   \n",
       "1       0.228531      0.002997         0.053695        0.001238       1   \n",
       "2       0.215722      0.006513         0.043921        0.001008       2   \n",
       "3       0.231588      0.004199         0.054353        0.000959       2   \n",
       "4       0.225223      0.008298         0.044203        0.001582       4   \n",
       "5       0.238724      0.004569         0.057497        0.002302       4   \n",
       "6       0.224928      0.008383         0.045405        0.001535       8   \n",
       "7       0.231566      0.002896         0.053920        0.001242       8   \n",
       "8       0.220163      0.006604         0.047966        0.005303      16   \n",
       "9       0.245850      0.011305         0.056053        0.003503      16   \n",
       "\n",
       "  param_kernel                         params  split0_test_score  \\\n",
       "0       linear   {'C': 1, 'kernel': 'linear'}           0.845714   \n",
       "1          rbf      {'C': 1, 'kernel': 'rbf'}           0.868571   \n",
       "2       linear   {'C': 2, 'kernel': 'linear'}           0.828571   \n",
       "3          rbf      {'C': 2, 'kernel': 'rbf'}           0.868571   \n",
       "4       linear   {'C': 4, 'kernel': 'linear'}           0.828571   \n",
       "5          rbf      {'C': 4, 'kernel': 'rbf'}           0.868571   \n",
       "6       linear   {'C': 8, 'kernel': 'linear'}           0.828571   \n",
       "7          rbf      {'C': 8, 'kernel': 'rbf'}           0.868571   \n",
       "8       linear  {'C': 16, 'kernel': 'linear'}           0.828571   \n",
       "9          rbf     {'C': 16, 'kernel': 'rbf'}           0.868571   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
       "0           0.817143           0.873563           0.793103           0.816092   \n",
       "1           0.840000           0.885057           0.775862           0.816092   \n",
       "2           0.817143           0.862069           0.810345           0.821839   \n",
       "3           0.840000           0.890805           0.775862           0.821839   \n",
       "4           0.840000           0.850575           0.804598           0.816092   \n",
       "5           0.840000           0.879310           0.775862           0.821839   \n",
       "6           0.840000           0.850575           0.804598           0.816092   \n",
       "7           0.840000           0.879310           0.775862           0.821839   \n",
       "8           0.840000           0.850575           0.804598           0.816092   \n",
       "9           0.840000           0.879310           0.775862           0.821839   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  \n",
       "0         0.829123        0.027788                6  \n",
       "1         0.837117        0.038705                2  \n",
       "2         0.827993        0.018047                7  \n",
       "3         0.839415        0.039596                1  \n",
       "4         0.827967        0.016392                8  \n",
       "5         0.837117        0.036779                2  \n",
       "6         0.827967        0.016392                8  \n",
       "7         0.837117        0.036779                2  \n",
       "8         0.827967        0.016392                8  \n",
       "9         0.837117        0.036779                2  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "resdf = pd.DataFrame.from_dict(tuned_svm.cv_results_)\n",
    "pd.DataFrame.from_dict(tuned_svm.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=2)\n",
      "{'C': 2, 'kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "print(tuned_svm.best_estimator_)\n",
    "print(tuned_svm.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8173076923076923\n"
     ]
    }
   ],
   "source": [
    "print(tuned_svm.score(test_x_vectors, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I think you should just take the title to heart, and skip the content. Try that experience. It's a little bit like seeing a movie preview and then realizing that the full length film isn't going to give you that much more information.\n",
      "NEGATIVE\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['NEGATIVE'], dtype='<U8')"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_tuned_svm = tuned_svm.best_estimator_\n",
    "\n",
    "best_tuned_svm.fit(train_x_vectors, train_y)\n",
    "\n",
    "n=2\n",
    "print(test_x[n])\n",
    "print(test_y[n])\n",
    "\n",
    "best_tuned_svm.predict(test_x_vectors[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8173076923076923\n"
     ]
    }
   ],
   "source": [
    "print(best_tuned_svm.score(test_x_vectors, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 2,\n",
       " 'break_ties': False,\n",
       " 'cache_size': 200,\n",
       " 'class_weight': None,\n",
       " 'coef0': 0.0,\n",
       " 'decision_function_shape': 'ovr',\n",
       " 'degree': 3,\n",
       " 'gamma': 'scale',\n",
       " 'kernel': 'rbf',\n",
       " 'max_iter': -1,\n",
       " 'probability': False,\n",
       " 'random_state': None,\n",
       " 'shrinking': True,\n",
       " 'tol': 0.001,\n",
       " 'verbose': False}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_tuned_svm.get_params(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['POSITIVE', 'NEUTRAL', 'NEGATIVE']\n",
      "[0.82075472 0.         0.81372549]\n",
      "[0.82075472 0.         0.81372549]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sinjoy/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1493: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  average, \"true nor predicted\", 'F-score is', len(true_sum)\n",
      "/home/sinjoy/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1493: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  average, \"true nor predicted\", 'F-score is', len(true_sum)\n"
     ]
    }
   ],
   "source": [
    "# F1 Scores\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "labels=[Sentiment.POSITIVE, Sentiment.NEUTRAL, Sentiment.NEGATIVE]\n",
    "print(labels)\n",
    "\n",
    "print(f1_score(test_y, tuned_svm.predict(test_x_vectors), average=None, labels=labels))\n",
    "print(f1_score(test_y, best_tuned_svm.predict(test_x_vectors), average=None, labels=labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./models/senti_clf_svm_c2_rbf.pkl', 'wb') as f:\n",
    "    pickle.dump(best_tuned_svm, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./models/senti_clf_svm_c2_rbf.pkl', 'rb') as f:\n",
    "    loaded_svm_clf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Although the Navajo element was interesting as background and provided a view as to life around a reservation, with its cultural nuances, the story itself was very basic. There were limited twists, if any, and the ending left me feeling like the reading journey was uneventful. Disappointing overall.\n",
      "NEGATIVE\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['NEGATIVE'], dtype='<U8')"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 42\n",
    "print(test_x[n])\n",
    "print(test_y[n])\n",
    "loaded_svm_clf.predict(test_x_vectors[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
